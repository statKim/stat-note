\chapter{Risk, Sufficiency, Completeness, and Ancillarity}

%% # 3.2
\section{Problem 3.2}
Suppose data $X_1, \dots, X_n$ are independent with 
$$ P_\theta(X_i \le x) = x^{t_i\theta}, ~~~~~ x \in (0, 1). $$
where $\theta > 0$ is the unknown parameter, and $t_1, \dots, t_n$ are known positive constants.
Find a one-dimensional sufficient statistic $T$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Take a first derivative for the above cdf, then the pdf of $X$ be $p_\theta(x) = t_i\theta x_i^{t_i\theta-1}$.
	Then, the joint density is
	$$ p_\theta(x_1,\dots,x_n) = \prod_{i=1}^n t_i\theta x_i^{t_i\theta-1} = \theta^n \left( \prod_{i=1}^n x_i^{t_i} \right)^\theta \frac{\prod_{i=1}^n t_i}{\prod_{i=1}^n x_i}. $$
	By factorization theorem, the sufficient statistic for $\theta$ is
	$$T = \prod_{i=1}^n x_i^{t_i}.$$	
\end{proof}


%% # 3.3
\section{Problem 3.3}
An object with weight $\theta$ is weighted on scales with different precision.
The data $X_1,\dots,X_n$ are independent, with $X_i \sim N(\theta, \sigma_i^2), ~ i=1,\dots, n$, with the standard deviations $\sigma_1,\dots, \sigma_n$ known constants.
Use sufficiency to suggest a weighted average of $X_1,\dots,X_n$ to estimate $\theta$.
(A weighted average would have form $\sum_{i=1}^n w_iX_i$, where the $w_i$ are positive and sum to one.)

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	The pdf of $X_i$ is 
	$$ p_\mu(x_i) = \frac{1}{\sqrt{2\pi}\sigma_i}e^{-\frac{1}{2\sigma_i^2}(x_i-\theta)^2} = \frac{1}{\sqrt{2\pi}\sigma_i} \exp\left[ \frac{\theta x_i}{\sigma_i^2} - \frac{x_i^2}{2\sigma_i^2} - \frac{\theta^2}{2\sigma_i^2} \right]. $$
	Then, the joint density is
	$$ p_\mu(x_1,\dots,x_n) = \frac{1}{(2\pi)^{n/2}\prod_i \sigma_i}\exp\left[ \sum_i\frac{\theta x_i}{\sigma_i^2} - \sum_i\frac{x_i^2}{2\sigma_i^2} - \sum_i\frac{\theta^2}{2\sigma_i^2} \right]. $$
	By factorization theorem, 
	$$T = \sum_{i=1}^n \frac{x_i}{\sigma_i^2}.$$
	To estimate $\theta$ using a weighted average form, let 
	$$w_i = \frac{T}{\sum_{i=1}^n \sigma_i^{-2}},$$
	then the weighted average of $X_1,\dots, X_n$ be an estimator for $\theta$ satisfied the sufficiency.
\end{proof}


%% # 3.4
\section{Problem 3.4}
Let $X_1,\dots,X_n$ be a random sample from an arbitrary discrete distribution $P$ on $\{1,2,3\}$.
Find a two-dimensional sufficient statistic.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Let $p_i = P(\{i\})$ and $n_i(x) = \sum_{j=1}^n  1_{\{ x_j = i \}}(x)$ for $i=1,2,3$.
	Then, $n_1(x) + n_2(x) + n_3(x) = n$.
	Since $X_i$s are i.i.d., the joint pdf is
	\begin{align*}
		P(X_1=x_1, \dots, X_n = x_n) &= p_1^{n_1(x)}p_2^{n_2(x)}p_3^{n_3(x)} \\
							      &= p_1^{n_1(x)}p_2^{n_2(x)}p_3^{n - n_1(x)-n_2(x)}.
	\end{align*}
	By factorization theorem, the sufficient statistic $T = (n_1,n_2)$.
\end{proof}


%% # 3.6
\section{Problem 3.6}
The beta distribution with parameters $\alpha > 0$ and $\beta > 0$ has density
$$ f_{\alpha,\beta}(x) = \begin{cases}
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}, ~~~x\in(0,1), \\
0,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\text{otherwise.}
\end{cases} $$
Suppose $X_1, \dots, X_n$ are i.i.d. from a beta distribution.
\begin{enumerate}
	\item[a)] Determine a minimal sufficient statistic (for the family of joint distributions) if $\alpha$ and $\beta$ vary freely.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			The joint density
			\begin{align*}
				f_{\alpha,\beta}(x_1,\dots,x_n) &= \exp\left[ \sum_i (\alpha-1)\log x_i + \sum_i (\beta-1)\log(1-x_i) + n\log \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \right] \\
									       &= \exp\left[ \sum_i \alpha \log x_i + \sum_i \beta \log(1-x_i) + n\log \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} - \sum_i\log x_i(1-x_i) \right],
			\end{align*}
			is the exponential family and full-rank.
			Then,
			$$ T = \left( \sum_{i=1}^n \log X_i, \sum_{i=1}^n \log(1-X_i) \right) $$
			is complete. \\
			By factorization theorem, $T$ is also sufficient. \\
			Therefore, $T$ is the minimal sufficient statistic for $(\alpha, \beta)$. (See Theorem 3.19) 
		\end{proof}
		
	\item[b)] Determine a minimal sufficient statistic if $\alpha = 2\beta$.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Plug into the joint density,
			\begin{align*}
				f_{\beta}(x_1,\dots,x_n) &= \exp\left[ \sum_i 2\beta \log x_i + \sum_i \beta \log(1-x_i) + n\log \frac{\Gamma(2\beta+\beta)}{\Gamma(2\beta)\Gamma(\beta)} - \sum_i\log x_i(1-x_i) \right] \\
									       &= \exp\left[ \beta \sum_i \left( \log x_i^2 + \log(1-x_i) \right) + n\log \frac{\Gamma(3\beta)}{\Gamma(2\beta)\Gamma(\beta)} - \sum_i\log x_i(1-x_i) \right]
			\end{align*}
			is the one-parameter exponential family and full-rank.
			Thus,
			$$T = \sum_i \left( \log x_i^2 + \log(1-x_i) \right) = 2T_1 +T_2$$
			is complete, and also sufficient by factorization theorem. \\
			Therefore, $T$ is the minimal sufficient statistic for $\beta$.
		\end{proof}
		
	\item[c)] Determine a minimal sufficient statistic if $\alpha = \beta^2$.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Now, the joint density is
			\begin{align*}
				p_{\beta}(x_1,\dots,x_n) &= \exp\left[ \sum_i (\beta^2-1) \log x_i + \sum_i (\beta-1) \log(1-x_i) + n\log \frac{\Gamma(2\beta+\beta)}{\Gamma(2\beta)\Gamma(\beta)}\right] \\
								     &= \exp\left[ (\beta^2-1) T_1(x) + (\beta-1) T_2(x) + n\log \frac{\Gamma(\beta^2+\beta)}{\Gamma(\beta^2)\Gamma(\beta)}\right].
			\end{align*}
			Suppose $p_\theta(x) \propto_\theta p_\theta(y)$. Then W.L.O.G. we consider 2 cases as follows:
			$$ \frac{p_1(y)}{p_1(x)} = \frac{p_2(y)}{p_2(x)} \Longrightarrow \frac{p_2(x)}{p_1(x)} = \frac{p_2(y)}{p_1(y)}, $$
			$$ \frac{p_1(y)}{p_1(x)} = \frac{p_3(y)}{p_3(x)} \Longrightarrow \frac{p_3(x)}{p_1(x)} = \frac{p_3(y)}{p_1(y)}. $$
			Since $p_1(x) = p_1(y) = 1$, from the above equation,
			$$ 3T_1(x) + T_2(x) + n\log\frac{\Gamma(2^2+2)}{\Gamma(2^2)\Gamma(2)} = 3T_1(y) + T_2(y) + n\log\frac{\Gamma(2^2+2)}{\Gamma(2^2)\Gamma(2)}, $$
			$$ 8T_1(x) + 2T_2(x) + n\log\frac{\Gamma(3^2+3)}{\Gamma(3^2)\Gamma(3)} = 8T_1(y) + 2T_2(y) + n\log\frac{\Gamma(3^2+3)}{\Gamma(3^2)\Gamma(3)}. $$
			These 2 equations imply $T(x) = T(y)$. \\
			Therefore, $T$ is the minimal sufficient statistic by Theorem 3.11.
		\end{proof}
\end{enumerate}


%% # 3.7
\section{Problem 3.7}
{\em Logistic regression}.
Let $X_1, \dots, X_n$ be independent Bernoulli variables, with $p_i = P(X_i=1), ~ i=1,\dots,n$.
Let $t_1,\dots,t_n$ be a sequence of known constants that are related to the $p_i$ via
$$ \log\frac{p_i}{1-p_i} = \alpha + \beta t_i, $$
where $\alpha$ and $\beta$ are unknown parameters.
Determine a minimal sufficient statistic for the family of joint distributions.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	From the above equation, 
	$$p_i = \frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}}.$$
	Then, the density of $X_i$ is
	\begin{align*}
		 p_{\alpha,\beta}(x_i) &= p_i^{x_i}(1-p_i)^{1-x_i} \\
		 				 &= \left( \frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}} \right)^{x_i} \left( \frac{1}{1+e^{\alpha+\beta t_i}} \right)^{1-x_i} \\
						 &= \exp\left[ x_i(\alpha+\beta t_i) - x_i\log(1+e^{\alpha+\beta t_i}) - (1-x_i)\log(1+e^{\alpha+\beta t_i}) \right] \\
						 &= \exp\left[ \alpha x_i + \beta x_it_i - \log(1+e^{\alpha+\beta t_i}) \right].
	\end{align*}
	Therefore, we can easily obtain the following sufficient statistic by factorization theorem for the joint density,
	$$T = \left( \sum_{i=1}^n X_i, \sum_{i=1}^n X_it_i \right).$$
	And also, the joint density is exponential family and full-rank. Thus $T$ is complete. \\
	$\therefore$ $T$ is minimal sufficient.
\end{proof}


%% # 3.8
\section{Problem 3.8}
The multinomial distribution, derived later in Section 5.3, is a discrete distribution with mass function
$$ \frac{n!}{x_1!\times \cdots \times x_s!}p_1^{x_1}\times \cdots \times p_s^{x_s},$$
where $x_1,\dots, x_s$ are non-negative integers summing to $n$, where $p_1,\dots, p_s$ are non-negative probabilities summing to one, and $n$ is the sample size.
Let $N_{11},N_{21},N_{22}$ have a multinomial distribution with $n$ trials and success probabilities $p_{11},p_{12},p_{21},p_{22}$.
(A common model for a two-by-two contingency table.)
\begin{enumerate}
	\item[a)] Give a minimal sufficient statistic if the success probabilities vary freely over the unit simplex in $\bbR^4$.
		(The unit simplex in $\bbR^p$ is the set of all vectors with non-negative entries summing to one.)
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			The density be the exponential family as follows:
			\begin{align*}
				p(\mathbf{N}) &= \frac{n!}{N_{11}! N_{12}! N_{21}! N_{22}!} p_{11}^{N_{11}}p_{12}^{N_{12}}p_{21}^{N_{21}} p_{22}^{N_{22}} \\
				       &= \frac{n!}{N_{11}! N_{12}! N_{21}! N_{22}!} \exp\left[ N_{11}\log p_{11} + N_{12}\log p_{12} + N_{21}\log p_{21} + N_{22}\log p_{22}  \right] \tag{$*$} \\
				       &= \frac{n!}{N_{11}! N_{12}! N_{21}! N_{22}!} \exp\left[ N_{11}\log p_{11} + N_{12}\log p_{12} + N_{21}\log p_{21} \right. \\
				       &~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left. + (n- N_{11} - N_{12} - N_{21})\log(1-p_{11} - p_{12}- p_{21})  \right] \\
				       &= \frac{n!}{N_{11}! N_{12}! N_{21}! N_{22}!} \exp\left[ N_{11}\log \left(\frac{p_{11}}{1-p_{11} - p_{12}- p_{21}}\right) + N_{12}\log \left(\frac{p_{12}}{1-p_{11} - p_{12}- p_{21}}\right) \right. \\
				       &~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left. + N_{21}\log \left(\frac{p_{21}}{1-p_{11} - p_{12}- p_{21}}\right) + n\log(1-p_{11} - p_{12}- p_{21})  \right].
			\end{align*}
			Since it is the exponential family of full-rank, $T=(N_{11}, N_{12}, N_{21})$ is complete, and also sufficient by factorization theorem.
			Thus $T=(N_{11}, N_{12}, N_{21})$ is minimal sufficient. \\
			If we use the equation ($*$), the minimal sufficient statistic is  $T=(N_{11}, N_{12}, N_{21}, N_{22})$.
		\end{proof}
		
	\item[b)] Give a minimal sufficient statistic if the success probabilities are constrained so that $p_{11}p_{22} = p_{12}p_{21}$.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			{\color{blue} 확실하지 않음...} \\
			From the constraint, plug in $p_{11} = (p_{12}p_{21})/p_{22}$ and denote $C$ is the term which is not depend on parameters,
			\begin{align*}
				p(\mathbf{N}) &= C p_{11}^{N_{11}}p_{12}^{N_{12}}p_{21}^{N_{21}} p_{22}^{N_{22}} \\
						     &= C \left( \frac{p_{12}p_{21}}{p_{22}} \right)^{N_{11}}p_{12}^{N_{12}}p_{21}^{N_{21}} p_{22}^{N_{22}} \\
						     &= C p_{12}^{N_{11}+N_{12}} p_{21}^{N_{11}+N_{21}} p_{22}^{N_{22}-N_{11}} \\
						     &= C p_{12}^{N_{11}+N_{12}} p_{21}^{N_{11}+N_{21}} (1-p_{11}-p_{12} - p_{21})^{n-N_{12} - N_{21}-2N_{11}} \\
				       		     &= C \exp\left[ (N_{11} + N_{12})\log \left(\frac{p_{12}}{1-p_{11} - p_{12}- p_{21}}\right) \right. \\
				      		     &~~~~~~~~~~~~~\left. + (N_{11}+N_{21})\log \left(\frac{p_{21}}{1-p_{11} - p_{12}- p_{21}}\right) + n\log(1-p_{11} - p_{12}- p_{21})  \right].
			\end{align*}
			Since it is the exponential family of full-rank, $T=(N_{11} + N_{12}, N_{11}+N_{21})$ is complete, and also sufficient by factorization theorem.
			Thus $T=(N_{11} + N_{12}, N_{11}+N_{21})$ is minimal sufficient.
		\end{proof}
\end{enumerate}


%% # 3.9
\section{Problem 3.9}
Let $f$ be a positive integrable function on $(0,\infty)$. Define
$$ c(\theta) = 1/\int_\theta^\infty f(x)dx, $$
and take $p_\theta(x) = c(\theta)f(x)$ for $x > \theta$, and $p_\theta(x) = 0$ for $x \le \theta$.
Let $X_1,\dots,X_n$ be i.i.d. with common density $p_\theta$.
\begin{enumerate}
	\item[a)] Show that $M=\min\{X_1,\dots,X_n\}$ is sufficient.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			The joint density is
			\begin{align*}
				p_\theta(x_1,\dots, x_n) &= c(\theta)^n \prod_i f(x_i) \\
								     &= c(\theta)^n \prod_i f(x_i) \cdot 1_{\min\{x_1,\dots, x_n\} > \theta }.
			\end{align*}
			By factorization theorem, $M = \min\{X_1,\dots,X_n\}$ is sufficient.
		\end{proof}
		
	\item[b)] Show that $M$ is minimal sufficient.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Suppose $p_\theta(x) \propto_\theta p_\theta(y)$.
			Then, the region of 0 value should be equal.
			Thus $T(x) = T(y)$ should be satisfied, therefore $T=M$ is minimal sufficient.
		\end{proof}
\end{enumerate}


%% # 3.10
\section{Problem 3.10}
Suppose $X_1,\dots,X_n$ are i.i.d. with common density $f_\theta(x) = (1+\theta x)/2, ~ |x| < 1; ~ f_\theta(x) = 0,$ otherwise, where $\theta \in [-1,1]$ is an unknown parameter.
Show that the order statistics are minimal sufficient.
(Hint: A polynomial of degree $n$ is uniquely determined by its value on a grid of $n+1$ points.)

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	The joint density is
	\begin{align*}
		f_\theta(x_1,\dots,x_n) &= 2^{-n}(1+\theta x_1)\cdots(1+\theta x_n) \\
						   &= 2^{-n}(1+\theta x_{(1)})\cdots(1+\theta x_{(n)}) 1_{\{-1 < x_{(1)} <\cdots < x_{(n)} < 1\}}.
	\end{align*}
	By factorization theorem, the order statistic $T = (X_{(1)}, \dots, X_{(n)})$ is sufficient. \\
	Now, suppose $p_\theta(x) \propto_\theta p_\theta(y)$.
	Since the joint density is the $n$-th order polynomials(다항식) with root(근) $-1/x_{(i)}$, the roots of $p_\theta(x)$ and $p_\theta(x)$ should be equal which implies $T(x) = T(y)$.
	Therefore, $T = (X_{(1)}, \dots, X_{(n)})$  is minimal sufficient.
\end{proof}


%% # 3.16
\section{Problem 3.16}
Let $X_1,\dots, X_n$ be a random sample from an absolutely continuous distribution with density
$$ f_\theta(x) = \begin{cases}
2x/\theta^2, ~~~x \in (0,\theta), \\
0, ~~~~~~~~~ \text{otherwise}.
\end{cases} $$
\begin{enumerate}
	\item[a)] Find a one-dimensional sufficient statistic $T$.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			The joint density is factorized as follows:
			$$ f_\theta(x_1,\dots,x_n) = 2^n\frac{x_1\cdots x_n}{\theta^{2n}} = \frac{2^n}{\theta^{2n}}x_{(1)}\cdots x_{(n)} 1_{x_{(1)} > 0} 1_{x_{(n)} < \theta}. $$
			By factorization theorem, $T = \max\{ X_1, \dots, X_n \}$ is the sufficient statistic for $\theta$.
		\end{proof}
		
	\item[b)] Determine the density of $T$.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Since $T = \max\{ X_1, \dots, X_n \}$,
			\begin{align*}
				P(T \le t) &= \prod_{i=1}^n P(X_i \le t) \\
					      &~~\text{Note that } P(X_i \le t) = \int_0^t 2x/\theta^2 dx = t^2/\theta^2. \\
					      &= \frac{t^{2n}}{\theta^{2n}}
			\end{align*}
			Taking a derivative with respect to $t$, the density of $T$ is
			$$ p_\theta(t) = \frac{2n}{\theta^{2n}}t^{2n-1}, ~~ t \in (0,\theta). $$
		\end{proof}
		
	\item[c)] Show directly that $T$ is complete.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Suppose $E_\theta(T) = c, ~ \forall \theta > 0$. Then,
			\begin{align*}
				&E_\theta\left[ f(T)-c \right] = \frac{2n}{\theta^{2n}}\int_0^\theta [f(t)-c] t^{2n-1} dt = 0 \\
				&\Rightarrow ~~ [f(t)-c]t^{2n-1}=0 ~~\text{ a.e.} ~ 0<t<\theta. \\
				&\Rightarrow ~~ f(T) = c.
			\end{align*}
			Therefore, $T$ is complete.
			 
			 \underline{Other solution} \\
			 \begin{align*}
				&E_\theta f(T) = \frac{2n}{\theta^{2n}}\int_0^\theta f(t) t^{2n-1} dt = c \\
				&\Rightarrow ~~ \int_0^\theta f(t) t^{2n-1} dt = c \theta^{2n}/2n, ~~ \forall \theta > 0 \\
				&\Rightarrow ~~ f(\theta)\theta^{2n-1} = c\theta^{2n-1} ~~\text{ a.e. } \theta \\
				&\Rightarrow ~~ f(t) = c ~~ \text{ a.e. } t.
			\end{align*}
		\end{proof}
\end{enumerate}


%% # 3.17
\section{Problem 3.17}
Let $X, X_1,X_2,\dots$ be i.i.d. from an exponential distribution with failure rate $\lambda$ (introduced in Problem 1.30).
\begin{enumerate}
	\item[a)] Find the density of $Y=\lambda X$.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Note that the density of $X$ is $p_\lambda(x) = \lambda e^{-\lambda x}, ~ \lambda > 0$, and its cdf is easily obtained by integration, $ P(X \le x) = 1-e^{-\lambda x}$.
			Thus the cdf of $Y$ is
			$$ P(Y \le y) = P(X \le y/\lambda) = 1-e^{-y}, ~ y > 0, $$
			and the density is
			$$ p(y) = e^{-y}, ~ y>0. $$
		\end{proof}
		
	\item[b)] Let $\overline{X} = (X_1 + \cdots + X_n) / n$. Show that $\overline X$ and $(X_1^2 + \cdots + X_n^2)/\overline X^2$ are independent.
		\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			The joint density of $X_1, \dots, X_n$ is
			$$ p_\lambda(x_1,\dots,x_n) = \lambda^ne^{-\lambda\sum_i x_i} = \lambda^n e^{-n\lambda \bar x}, $$
			is the full rank exponential family, and by factorization theorem, $T= \overline X$ is the complete sufficient statistic. \\
			Now, let $Y=\lambda X$, then  
			$$ V = \frac{(Y_1^2 + \cdots + Y_n^2)}{\overline Y^2} = \frac{(X_1^2 + \cdots + X_n^2)}{\overline X^2}. $$
			Since $Y$ does not depend on parameter $\lambda$, $V$ is ancillary for $P_\lambda$. \\
			$\therefore$ By Basu's theorem, $T$ and $V$ are independent.
		\end{proof}
\end{enumerate}


%% # 3.29
\section{Problem 3.29}
Find a function on $(0,\infty)$ that is bounded and strictly convex.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	$f(x) = e^{-x}$ is bounded on range $(0,1)$ and strictly convex. \\
	Or $f(x) = 1/(x+1)$ is bounded on range $(0,1)$ and strictly convex.
\end{proof}


%% # 3.30
\section{Problem 3.30}
Use convexity to show that the canonical parameter space $\Xi$ of a one-parameter exponential family must be an interval.
Specifically, show that if $\eta_0 <\eta < \eta_1$, and if $\eta_0$ and $\eta_1$ both lie in $\Xi$, then $\eta$ must lie in $\Xi$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Note that if a random variable $X$ is a canonical exponential family, then it has the form,
	$$ p_\eta(x) = \exp[\eta T(x) - A(\eta)]h(x), ~~ \eta \in \Xi. $$
	Since $\eta_0 < \eta < \eta_1$, we define $\eta = \gamma \eta_0 + (1-\gamma)\eta_1$ for some $\gamma \in (0,1)$.
	Since the exponential function is convex, 
	$$ \exp\big[ (\gamma \eta_0 + (1-\gamma)\eta_1)T(x) \big] = e^{\eta T(x)} < \gamma e^{\eta_0 T(x)} + (1-\gamma)e^{\eta_1T(x)}. $$
	From the definition of exponential family, $h(x) \ge 0$, and for the positive function, the inequality sign does not change by integration against $\mu$. Thus, following inequality is satisfied:
	$$ \int e^{\eta T(x)}h(x) d\mu(x) < \gamma \int e^{\eta_0 T(x)}h(x)d\mu(x) + (1-\gamma) \int e^{\eta_1T(x)}h(x)d\mu(x). $$
	Because it should be satisfied the exponential family, RHS is finite. ($\because$ RHS $= \gamma e^{A(\eta_0)} + (1-\gamma)e^{A(\eta_1)} < \infty$.) 
	Therefore, $\eta$ must lie in $\Xi$.
\end{proof}


%% # 3.31
\section{Problem 3.31}
Let $f$ and $g$ be positive probability densities on $\bbR$. Use Jensen's inequality to show that
$$ \int \log \left( \frac{f(x)}{g(x)} \right) f(x)dx > 0, $$
unless $f = g$ a.e. (If $f=g$, the integral equals zero.)
This integral is called the {\em Kullback-Liebler information}.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Suppose $X$ be a absolutely continuous random variable with density $f$.
	Now, define $Y= \frac{g(X)}{f(X)}$ , then
	$$ EY = \int \frac{g(x)}{f(x)}f(x) dx = \int g(x)dx = 1. $$
	Next, let $h(y) = \log\frac{1}{y} = -\log y$, then $h$ is strictly convex on $(0,\infty)$.
	Then, by Jensen's inequality,
	$$  Eh(Y)  = \int \log \left( \frac{f(x)}{g(x)} \right) f(x)dx \ge h\left( EY \right) = \log1 = 0. $$
	The equality holds when $Y$ is a constant. (then $Y=EY=1 \Rightarrow f(x)=g(x)$ a.e.) 
\end{proof}