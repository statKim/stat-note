% Chapter 2. Exponential Families
\chapter{Exponential Families}

%% # 2.1
\section{Problem 2.1}
Consider independent Bernoulli trials with success probability $p$ and let $X$ be the number of failures before the first success.
Then $P(X=x) = p(1-p)^x$, for $x = 0,1,\dots $, and $X$ has the geometric distribution with parameter $p$, introduced in Problem 1.17.
\begin{itemize}
	\item[a)] Show that the geometric distributions form an exponential family.
            	\begin{proof}[\underline{\textbf{Solution}}]
                		$$ P(X=x) = p(1-p)^x = \exp\big\{ x\log(1-p) - (-\log p) \big\}. $$
			$\therefore$ the geometric distribution is an exponential family.
            	\end{proof}
	
	\item[b)] Write the densities for the family in canonical form, identifying the canonical parameter $\eta$, and the function $A(\eta)$.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
                		Let $\eta(p) = \log(1-p).$ Then, $ P(X=x) = \exp\big\{ \eta x - \big(-\log(1-e^\eta)\big) \big\}. $ \\
			$\therefore A(\eta) = -\log(1-e^\eta)$ with $T(x) = x$.
            	\end{proof}
	
	\item[c)] Find the mean of the geometric distribution using a differential identity.
            	\begin{proof}[\underline{\textbf{Solution}}] 
                		$$ E_\eta(T) = A'(\eta) = \frac{e^\eta}{1-e^\eta} = \frac{1-p}{p}. $$
            	\end{proof}	
	
	\item[d)] Suppose $X_1, \dots, X_n$ are i.i.d. from a geometric distribution. Show that the joint distributions form an exponential family, and find the mean and variance of $T$.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
                		$$ P(X_1 = x_1,\dots, X_n = x_n) = \prod_{i=1}^n P(X_i = x_i) = p^n(1-p)^{\sum_{i=1}^n x_i} = \exp\big\{ \sum_ix_i\log(1-p) - (-n\log p) \big\}. $$
		Therefore, the joint distribution is also exponential family.\\
		Now, let $\eta = \log(1-p)$. Then, the canonical exponential family is obtained with $A(\eta) = -n\log(1-e^\eta)$ and $T=\sum_i x_i$. 
		$$ ET = \kappa_1 = A'(\eta) = \frac{ne^\eta}{1-e^\eta} = \frac{n(1-p)}{p}, $$
		$$ Var(T) = \kappa_2 = A''(\eta) = \frac{ne^\eta(1-e^\eta)+ne^{2\eta}}{(1-e^\eta)^2} = \frac{np(1-p)+n(1-p)^2}{p^2} = \frac{n(1-p)}{p^2}. $$
            	\end{proof}
\end{itemize}


%% # 2.2
\section{Problem 2.2}
Determine the canonical parameter space $\Xi$, and find densities for the one-parameter exponential family with $\mu$ Lebesgue measure on $\bbR^2$, $h(x,y) = \exp\big[ -(x^2+y^2)/2 \big] / (2\pi)$, and $T(x,y) = xy$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	By definition of the canonical exponential family, the pdf be represented as
	$$ p(x,y) = \exp\big(\eta T(x,y) - A(\eta)\big) h(x,y) = \exp\big\{\eta xy - A(\eta) - (x^2+y^2)/2 \big\} / (2\pi).$$
	Thus,
	\begin{align*}
		\int p(x,y)d\mu(x,y) = 1 &\Longleftrightarrow \int \int \exp\big\{\eta xy - A(\eta) - (x^2+y^2)/2 \big\} / (2\pi) dxdy = 1 \\
						 &\Longleftrightarrow e^{A(\eta)} = \int\int \frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{1}{2}(x-\eta y)^2\right\}dx \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(y^2-\eta^2y^2)}dy \\
						 &\Longleftrightarrow e^{A(\eta)} = \frac{1}{\sqrt{2\pi}}\int e^{-\frac{1}{2} y^2(1-\eta^2)}dy \underset{\underset{\text{the integral is finite } \Leftrightarrow |\eta|<1}{\uparrow}}{=} (1-\eta^2)^{-1/2}.
	\end{align*}
	$$ \therefore A(\eta) = -\frac{1}{2} \log(1-\eta^2) \text{ for } \eta \in \Xi = (-1, 1).$$
	$ \therefore $ The canonical exponential density is
	$$ \exp\big\{\eta xy + \frac{1}{2} \log(1-\eta^2) - (x^2+y^2)/2 \big\} / (2\pi). $$
\end{proof}


%% # 2.4
\section{Problem 2.4}
Find the natural parameter space $\Xi$ and densities $p_\eta$ for a canonical one-parameter exponential family with $\mu$ Lebesgue measure on $\bbR$, $T_1(x) = \log x$, and $h(x) = (1-x)^2, ~ x\in (0,1)$, and $h(x) = 0, ~ x\not\in (0, 1)$.

\begin{proof}[\underline{\textbf{Solution}}]
	\begin{align*}
		 e^{A(\eta)} &= \int_0^1 e^{\eta\log x} (1-x)^2 dx \\
		 		  & \text{Let } t = \log x \Rightarrow dt = 1/x dx = e^{-t}dx, ~ -\infty < t < 0 \\
		 		  &= \int_{-\infty}^0 e^{\eta t}(1-e^t)^2e^t dt \\
				  &= \int_{-\infty}^0 \left( e^{t(\eta+1)} - 2e^{t(\eta+2)} + e^{t(\eta+3)} \right) dt \\
				  &= \left( \frac{1}{\eta+1} e^{t(\eta+1)}   -\frac{2}{\eta+2}e^{t(\eta+2)} +\frac{1}{\eta+3} e^{t(\eta+3)} \right) \Big|_{-\infty}^0 \\
				  &= \frac{1}{\eta+1}  -\frac{2}{\eta+2} +\frac{1}{\eta+3}\\
				  &= \frac{2}{(\eta+1)(\eta+2)(\eta+3)}, ~~~ \eta \in \Xi = (-1, \infty).
	\end{align*}
	($\because$ For a finite integral, $\eta+1 >0, ~ \eta+2 > 0,$ and $\eta + 3>0$ $\Rightarrow \eta > -1$.)
	$$ \therefore p_\eta(x) =  \exp\big(\eta T(x) - A(\eta)\big) h(x) =  \frac{1}{2}(\eta+1)(\eta+2)(\eta+3)(1-x)^2x^{\eta}, ~~~ x \in (0,1). $$
\end{proof}


%% # 2.5
\section{Problem 2.5}
Find the natural parameter space $\Xi$ and densities $p_\eta$ for a canonical one-parameter exponential family with $\mu$ Lebesgue measure on $\bbR$, $T_1(x) = -x$, and $h(x) = e^{-2\sqrt{x}}/\sqrt{x}, ~ x > 0$, and $h(x) = 0, ~ x \le 0$. (\underline{Hint}: After a change of variables, relevant integrals will look like integrals against a normal density. You should be able to express the answer using $\Phi$, the standard normal cumulative distribution function.)
Also, determine the mean and variance for a variable $X$ with this density.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	\begin{align*}
		 e^{A(\eta)} &= \int_0^\infty e^{-\eta x - 2\sqrt{x}}\frac{1}{\sqrt{x}} dx \\
		 		  & \text{To the integral is finite, } \eta \in \Xi = [0, \infty), \\
		 		  & \text{Let } t = \sqrt x \Rightarrow t^2=x, ~ 2tdt = dx \\
				  &= \int_0^\infty \frac{1}{t} e^{-\eta t^2-2t}2t dt \\
				  &= 2\int_0^\infty e^{-\eta\left(t^2+\frac{2}{\eta} t\right)}dt \\
				  &= 2\int_0^\infty e^{-\frac{2\eta}{2}\left(t+\frac{1}{\eta}\right)^2} dt \cdot e^{-\eta (-\frac{1}{\eta^2})} \\
				  & \text{Since inner term of integration } \sim N( -1/\eta, 1/2\eta ), \\
				  &= 2\sqrt{2\pi}/\sqrt{2\eta} \cdot e^{\frac{1}{\eta}}\Phi\left(\frac{-1/\eta}{\sqrt{1/(2\eta)}} \right) \\
				  &= \frac{2\sqrt{\pi}}{\sqrt\eta}e^{1/\eta}\Phi( -\sqrt{2/\eta} ) \\
		\Rightarrow A(\eta) &= \log 2\sqrt\pi -\frac{1}{2}\log\eta + \frac{1}{\eta} + \log\Phi\left(-\sqrt{\frac{2}{\eta}}\right).
	\end{align*}
	$$ \therefore p_\eta(x) = \exp\big\{ -\eta x -2\sqrt{x} - A(\eta) \big\} \frac{1}{\sqrt x}, ~~~ x > 0. $$
	The mean of $X$ is
	$$ E_\eta(X) = -E_\eta T = -A'(\eta) = \frac{1}{2\eta} + \frac{1}{\eta^2} - \frac{\frac{\sqrt{2}}{2}\eta^{-3/2} \phi\left(-\sqrt{\frac{2}{\eta}}\right)}{\Phi\left(-\sqrt{\frac{2}{\eta}}\right)}. $$
\end{proof}


%% # 2.6
\section{Problem 2.6}
Find the natural parameter space $\Xi$ and densities $p_\eta$ for a canonical two-parameter exponential family with $\mu$ counting measure on $\{0,1,2\}$, $T_1(x) = x, ~ T_2(x) = x^2$, and $h(x) = 1$ for $x \in \{0,1,2\}$.

\begin{proof}[\underline{\textbf{Solution}}]
	\begin{align*}
		e^{A(\eta)} &= \sum_{x=0}^2 e^{\eta_1x+\eta_2x^2} \\
				 &= 1 + e^{\eta_1 +\eta_2} + e^{2\eta_1 + 4\eta_2}.
	\end{align*}
	To the sum is finite, $\eta_1 + \eta_2 < \infty ~\& ~  2\eta_1+4\eta_2 < \infty \Rightarrow \eta \in \Xi = \bbR^2.$
	$$ \therefore p_\eta(x) = \exp\left[ \eta_1x + \eta_2x^2 -\left( 1+e^{\eta_1+\eta_2} + e^{2\eta_1+4\eta_2} \right) \right]. $$
\end{proof}


%% # 2.7
\section{Problem 2.7}
Suppose $X_1, \dots, X_n$ are independent geometric variables with $p_i$ the success probability for $X_i$.
Suppose these success probabilities are related to a sequence of "independent" variables $t_1,\dots, t_n$, viewed as known constants, through
$$ p_i = 1- \exp(\alpha + \beta t_i), ~~~~ i=1,\dots, n. $$
Show that the joint densities for $X_1, \dots, X_n$ form a two-parameter exponential family, and identify the statistics $T_1$ and $T_2$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Since $X_i \overset{\mathrm{ind}}{\sim} Geo(p_i)$,  then the joint density is represented by
	\begin{align*}
	 	\prod_{i=1}^n p(x_i) &= \prod_{i=1}^n (1-p_i)^{x_i}p_i = \exp[\sum_{i=1}^nx_i\log(1-p_i) + \sum_{i=1}^n\log p_i] \\
					       &= \exp[\sum_{i=1}^nx_i(\alpha + \beta t_i) + \sum_{i=1}^n\log(1-\exp(\alpha+\beta t_i))].
	\end{align*}
	$\therefore T_1 = \sum_{i=1}^n X_i, ~ T_2 = \sum_{i=1}^nt_iX_i.$
\end{proof}


%% # 2.8
\section{Problem 2.8}
Assume that $X_1, \dots, X_n$ are independent random variables with $X_i \sim N(\alpha + \beta t_i, 1)$, where $t_1,\dots, t_n$ are observed constants and $\alpha$ and $\beta$ are unknown parameters.
Show that the joint densities for $X_1, \dots, X_n$ form a two-parameter exponential family, and identify the statistics $T_1$ and $T_2$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	The joint density of $X_1,\dots, X_n$ is
	$$ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp\left[ -\frac{1}{2}(x_i - \alpha-\beta t_i)^2 \right] = (2\pi)^{-\frac{n}{2}} \exp \left[ \sum_i x_i(\alpha+\beta t_i) - \frac{1}{2} \sum_i (\alpha + \beta t_i) \right] \exp\left( -\sum_i \frac{x_i^2}{2}\right). $$
	$\therefore T_1 = \sum_{i=1}^n X_i, ~ T_2 = \sum_{i=1}^n t_iX_i.$
\end{proof}


%% # 2.9
\section{Problem 2.9}
Suppose that $X_1, \dots, X_n$ are independent Bernoulli variables (a random variable is Bernoulli if it only takes on values 0 and 1) with
$$ P(X_i = 1) = \frac{\exp(\alpha+\beta t_i)}{1 + \exp(\alpha + \beta t_i)}. $$
Show that the joint distribution for $X_1, \dots, X_n$ form a two-parameter exponential family, and identify the statistics $T_1$ and $T_2$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	The joint density of $X_1, \dots, X_n$ is
	\begin{align*}
		\prod_{i=1}^n p_i^{x_i}(1-p_i)^{1-x_i} &= \exp\left[ \sum_i x_i\log p_i + \sum_i (1-x_i)\log (1-p_i) \right] \\
									&= \exp \left[ \sum_i x_i\left(\alpha + \beta t_i - \log\left(1+e^{\alpha+\beta t_i}\right)\right) + \sum_i (1-x_i)\left(1-\log\left(1+e^{\alpha+\beta t_i}\right)\right) \right] \\
									&= \exp\left[ \sum_i x_i(\alpha + \beta t_i) - \sum_i \log\left( 1+e^{\alpha+\beta t_i} \right) \right].
	\end{align*}
	$\therefore T_1 = \sum_{i=1}^n X_i, ~ T_2 = \sum_{i=1}^n t_iX_i.$
\end{proof}


%% # 2.15
\section{Problem 2.15}
For an exponential family in canonical form, $ET_j = \partial A(\eta)/\partial \eta_j$.
This can be written in vector form as $ET = \nabla A(\eta)$.
Derive an analogous differential formula for $E_\theta T$ for an $s$-parameter exponential family that is not in canonical form.
Assume that $\Omega$ has dimension $s$. \\
\underline{Hint}: Differentiation under the integral sign should give a system of linear equations. Write these equations in matrix form.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Note that the exponential family form is
	\begin{align*}
		p_\eta(x) &= \exp\left[ \sum_{i=1}^s \eta_i T_i(x) - A(\eta) \right]h(x) \\
			       &= \exp\left[ \sum_{i=1}^s \eta_i(\theta)T_i(x) - B(\theta) \right]h(x) \\
			       &= p_\theta(x).
	\end{align*}
	Thus, 
	$$ e^{B(\theta)} = \int \exp\left[ \sum_{i=1}^s \eta_i(\theta)T_i(x) \right]h(x) d\mu(x). $$
	By differentiating with respect to $\theta_i$,
	$$ e^{B(\theta)} \frac{\partial B(\theta)}{\partial \theta_i} = \int \left( \sum_{j=1}^s \frac{\partial \eta_j(\theta)}{\partial \theta_i}T_j(x) \right) \exp\left[ \sum_{j=1}^s \eta_j(\theta)T_j(x) \right]h(x) d\mu(x). $$
	\begin{align*}
		\frac{\partial B(\theta)}{\partial \theta_i} &= \int \left( \sum_{j=1}^s \frac{\partial \eta_j(\theta)}{\partial \theta_i}T_j(x) \right) p_\theta (x) d\mu(x) \\
									   &= \sum_{j=1}^s \frac{\partial \eta_j(\theta)}{\partial \theta_i} E_\theta T_j, ~~~~ i = 1, \dots, s.
	\end{align*}
\end{proof}


%% # 2.17
\section{Problem 2.17}
Let $\mu$ denote counting measure on $\{1,2,\dots\}$.
One common definition for $\sum_{k=1}^\infty f(k)$ is $\limn \sum_{k=1}^n f(k)$, and another definition is $\int f d\mu$.
\begin{itemize}
	\item[a)] Use the dominated convergence theorem to show that the two definitions give the same answer when $\int |f| d\mu < \infty$.\\
		\underline{Hint}: Find functions $f_n, ~ n=1,2,\dots$, so that $\sum_{k=1}^n f(k) = \int f_n d\mu$.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
            		Let $f_n(k) = f(k), ~ \forall k \le n$, $f_n(k) = 0, ~ \forall k > n$.
			Then, $f_n \to f$ pointwise and $|f_n| \le |f|$.
			Also $f_n$ is simple and $\int f_n d\mu = \sum_{k=1}^n f(k)$. \\
			By D.C.T., $\int f_n d\mu \to \int f d\mu.$
            	\end{proof}

	\item[b)] Use the monotone convergence theorem, give in Problem 1.25, to show the definitions agree if $f(k) \ge 0$ for all $1,2,\dots$.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
            		Define $f_n$ and $f$ like part (a).
			Then, $0 \le f_1 \le f_2 \le \cdots$. \\
			By M.C.T., $\sum_{k=1}^n f(k) = \int f_nd\mu \to \int f d\mu$.
            	\end{proof}
	
	\item[c)] Suppose $\limn f(n) = 0$ and that $\int f^+d\mu = \int f^-d\mu = \infty$ (so that $\int f d\mu$ is undefined.)
		Let $K$ be an arbitrary constant. Show that the list $f(1), f(2), \dots$ can be rearranged to form a new list $g(1), g(2), \dots$ so that
		$$ \limn \sum_{k=1}^n g(k) = K. $$
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
			Take positive parts of the sequence $f$ until the sum is exceed $K$.
			And then, take negative parts of the sequence $f$ until the sum is below $K$.
			Repeat this procedure like the figure below. {\color{blue} (In the figure, $g(k)$ is typo. $\sum g(k)$ is correct.) }
			\begin{figure}[!h]
				\center
				\includegraphics[width = 0.3 \textwidth]{figure/2-17.jpeg}
			\end{figure}
			Then, the sum of the sequence goes to 0 for $k > k'$ where $k'$ is the number when the sum of the rearranged sequence exceed $K$, first.
			Let this sequence as $g$.
			Then, $\limn \sum_{k=1}^n g(k) \to K.$
            	\end{proof}
\end{itemize}


%% # 2.19
\section{Problem 2.19}
Let $p_n, n = 1, 2, \dots,$ and $p$ be probability densities with respect to a measure $\mu$, and let $P_n, n=1,2,\dots$, and $P$ be the corresponding probability measures.
\begin{itemize}
	\item[a)] Show that if $p_n(x) \rightarrow p(x)$ as $n \to \infty$, then $\int |p_n-p| d\mu \to 0$. \\
		\underline{Hint}: First use the fact that $\int(p_n-p)d\mu = 0$ to argue that $\int |p_n-p|d\mu = 2\int(p-p_n)^+d\mu$.
		Then use dominated convergence.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
            		Since $p_n$ and $p$ be probability densities, $\int (p_n-p)d\mu = \int p_n d\mu - \int p d\mu = 1-1= 0$.
			Since $p_n - p = (p-p_n)^+  - (p-p_n)^-$,
			$$ \int (p_n-p) d\mu = \int (p-p_n)^+ d\mu  - \int (p-p_n)^- d\mu = 0, $$
			$$\therefore \int (p-p_n)^+ d\mu  = \int (p-p_n)^- d\mu. $$
			
			Also $|p_n - p| = (p-p_n)^+ + (p-p_n)^-$, 
			$$ \int |p_n - p| d\mu = \int (p-p_n)^+ d\mu + \int (p-p_n)^- d\mu = 2\int (p-p_n)^+ d\mu. $$
			Also note that $|(p-p_n)^+| \le p \Rightarrow |(p-p_n)^+|$ is integrable. ($\because$ $p$ is a probability density $\Rightarrow p$ is integrable.)
			From the condition, $(p_n \to p) \Rightarrow \big(p(x) - p_n(x)\big)^+ \to 0$. \\
			By D.C.T., 
			$$ \int |p_n - p| d\mu = 2\int (p-p_n)^+ d\mu \to 0.$$
            	\end{proof}

	\item[b)] Show that $|P_n(A)-P(A)| \le \int|p_n - p|d\mu$. \\
		\underline{Hint}: Use indicators and the bound $|\int f d\mu| \le \int |f|d\mu$.
            	\begin{proof}[\underline{\textbf{Solution}}]
			\begin{align*}
				LHS &= \left| \int 1_A dP_n - \int 1_A dP \right| \\
				       &= \left|\int 1_A(p_n - p) d\mu\right| \\
				       &\le \int \left| 1_A(p_n - p) \right| d\mu \\
				       &\le \int \left| p_n - p \right| d\mu.
			\end{align*}
            	\end{proof}
\end{itemize}
Remark: Distributions $P_n, ~ n \ge 1$, are said to {\em converge strongly} to $P$ if $\sup_A|P_n(A) - P(A)| \to 0$.
The two parts above show that pointwise convergence of $p_n$ to $p$ implies strong convergence.
This was discovered by Scheff\'{e}.


%% # 2.22
\section{Problem 2.22}
Suppose $X$ is absolutely continuous with density
$$ p_\theta(x) = \begin{cases}
\frac{e^{-(x-\theta)^2/2}}{\sqrt{2\pi}\Phi(\theta)}, ~~~ x > 0, \\
0, ~~~~~~~~~~~~~~ \text{otherwise.}
\end{cases} $$
Find the moment generating function of $X$. Compute the mean and variance of $X$.

\begin{proof}[\underline{\textbf{Solution}}]
	$$ p_\theta(x) = \exp\left[ -\frac{1}{2}(x^2-2x\theta+\theta^2) - \log\Phi(\theta) \right]\frac{1}{\sqrt{2\pi}}, ~~ x > 0 $$
	is the exponential family with
	$T = X, ~ A(\theta) = \frac{\theta^2}{2} + \log\Phi(\theta).$
	Therefore, the moment generating function of $T=X$ is
	\begin{align*}
		M_X(u) &= \exp\left[ A(\theta + u) - A(\theta) \right] \\
			     &= \exp\left[ \frac{1}{2}(\theta+u)^2 - \frac{1}{2}\theta^2 \right] \Phi(\theta+u) / \Phi(\theta) \\
			     &= \exp\left( \theta u + \frac{1}{2}u^2 \right) \Phi(\theta+u) / \Phi(\theta).
	\end{align*}
	Meanwhile, the {\em c.g.f.} of $T=X$ for exponential family is $K_X(u) = A(\theta + u) - A(\theta)$.
	Thus, 
	$$ EX = K_X'(u) = A'(\theta) = \theta + \frac{\phi(\theta)}{\Phi(\theta)}, $$
	$$ \Var(X) = K_X''(u) = A''(\theta) = 1 + \frac{\phi '(\theta)\Phi(\theta) - \phi(\theta)^2}{\Phi(\theta)^2}. $$
	$$ \left( \because \phi(x) = e^{-x^2/2} / \sqrt{2\pi} \Rightarrow \phi'(x) = -x\phi(x). \right) $$
\end{proof}


%% # 2.23
\section{Problem 2.23}
Suppose $Z \sim N(0,1)$. Find the first four cumulants of $Z^2$. \\
\underline{Hint}: Consider the exponential family $N(0,\sigma^2)$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Let $X \sim N(0, \sigma^2)$. Then, its density is
	$$ p_\sigma(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{s\sigma^2}} = \frac{1}{\sqrt{2\pi}}\exp\left[ -\frac{1}{2\sigma^2}x^2-\log\sigma \right], $$
	and also be the exponential family. \\
	Reparameterize $\eta = -\frac{1}{2\sigma^2}$, then $T = X^2$ and $A(\eta) = \frac{1}{2}\log -\frac{1}{2\eta} = -\frac{1}{2} \log(-2\eta)$.
	Thus, the cumulative generating function of $T=X^2$ is
	\begin{align*}
		K_{X^2}(u) &= A(\eta+u) - A(\eta) \\
				  &= -\frac{1}{2}\left[ \log\big(-2(\eta+u)\big) + \log(-2\eta) \right].
	\end{align*}
	If $\sigma = 1$, then $\eta = -\frac{1}{2}$ and $X^2 = Z^2$.
	Therefore, $$ K_{Z^2}(u) = -\frac{1}{2}\left[ \log(-2u + 1)\right]. $$
	$$ K_{Z^2}'(u) = \frac{1}{1-2u}, ~~~ K_{Z^2}''(u) = \frac{2}{(1-2u)^2}, $$
	$$ K_{Z^2}^{(3)}(u) = 8(1-2u)^{-3}, ~~~ K_{Z^2}^{(4)}(u) = 48(1-2u)^{-4}.$$
	$\therefore$ the cumulants are 1, 2, 8, and 48, respectively.
\end{proof}


%% # 2.24
\section{Problem 2.24}
Find the first four cumulants of $T=XY$ when $X$ and $Y$ are independent standard normal variates.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Since $X$ and $Y$ are independent, the function of these r.v.s are also independent. (i.e. $g(X)$ and $g(Y)$ are independent for any function $g$.)
	Then, first 2 cumulants are
	$$ \kappa_1= ET = EXY = EXEY = 0, $$
	$$ \kappa_2 = E(T-ET)^2 = EX^2Y^2 = EX^2EY^2 = 1. $$
	
	For third cumulants, we need to obtain $EX^3$.
	\begin{align*}
		EX^3 &= \int x^3 \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx \\
			 & \text{Let } t = x^2/2 \Rightarrow xdx = dt \\
			 &= \frac{2}{\sqrt{2\pi}} \int_0^\infty te^{-t}dt \\
			 &= \frac{2}{\sqrt{2\pi}} \left[ -te^{-t}\big|_0^\infty + \int_0^\infty e^{-t}dt \right] \\
			 &= 0.
	\end{align*}
	Thus, the third cumulants is
	$$ \kappa_3 = E(T - ET)^3 = EX^3Y^3 = EX^3EY^3 = 0. $$
	
	Similarly, we need to obtain $EX^4$.
	{\color{blue} 자꾸 적분이 틀림... 다시 해보기 Gamma dist 형태 사용}
	\begin{align*}
		EX^4 &= \int x^4 \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx \\
			 & \text{Let } t = x^2/2 \Rightarrow xdx = dt \\
	\end{align*}

	$$ \kappa_4 = E(T-ET)^4 - 3\Var(T)^2 = EX^4EY^4 - 3\cdot 1^2 = 4. $$
\end{proof}


%% # 2.25
\section{Problem 2.25}
Find the third and fourth cumulants of the geometric distribution.

\begin{proof}[\underline{\textbf{Solution}}] 
	$$f_p(x) = (1-p)^xp = \exp\left[ x\log(1-p) + \log p \right]$$
	is the exponential family.
	Reparameterize $\eta = -\log(1-p)$ ($\Rightarrow p = 1-e^\eta$), then the above form be the canonical exponential family with $T=X$ and $A(\eta) = -\log(1-e^\eta)$.
	
	Thus, the cumulative generating function of $T=X$ is
	$$ K_X(u) = A(\eta + u) - A(\eta) = -\log(1-e^{\eta+u}) + \log(1-e^\eta).$$
	Then, the first cumulants is
	$$ K_X'(0) = A'(\eta) = \frac{e^\eta}{1-e^\eta} = \frac{1-p}{p}.$$
	The higher order cumulants are easily obtained by using the chain rule.
	By using the $p' = \frac{dp}{d\eta} = -e^\eta = p-1$,
	$$ K_X''(0) = A''(\eta) = \frac{d}{dp}A'(\eta) \frac{dp}{d\eta} = \frac{-p - (1-p)}{p^2}(p-1) = \frac{1}{p^2} - \frac{1}{p}, $$
	$$ K_X^{(3)}(0) = A^{(3)}(\eta) = \frac{d}{dp}A''(\eta) p' = -\frac{2p'}{p^3} + \frac{p'}{p^2} = -\frac{2-2p}{p^3} + \frac{p-1}{p^2} = \frac{2}{p^3} - \frac{3}{p^2} + \frac{1}{p}, $$
	$$ K_X^{(4)}(0) = A^{(4)}(\eta) = \frac{d}{dp}A^{(3)}(\eta) p' = -\frac{6p'}{p^4} + \frac{6p'}{p^3} - \frac{p'}{p^2} = \frac{6}{p^4}-\frac{12}{p^3}+\frac{7}{p^2}-\frac{1}{p}. $$
\end{proof}


%% # 2.26
\section{Problem 2.26}
Find the third cumulant and third moment of the binomial distribution with $n$ trials and success probability $p$.

\begin{proof}[\underline{\textbf{Solution}}] 
	$$ p(x) = {n \choose x} p^x(1-p)^{n-x} = {n \choose x}\left(\frac{p}{1-p}\right)^x(1-p)^n = \exp\left[  x\log\frac{p}{1-p} + n\log(1-p)\right] {n \choose x} $$
	is the exponential family for $\eta = \log\frac{p}{1-p}$ with $T=X$ and $A(\eta) = -n\log(1-p)$.
	To use the chain rule,
	$$ p = \frac{e^\eta}{1+e^\eta} \Rightarrow \frac{dp}{d\eta} = \frac{e^\eta(1+e^\eta)-(e^\eta)^2}{(1+e^\eta)^2} = p(1-p). $$
	By using the above fact,
	$$ \kappa_1 = A'(\eta) = \frac{d}{dp}A(\eta) \frac{dp}{d\eta} = \frac{n}{1-p}p(1-p) = np, $$
	$$ \kappa_2 = A''(\eta) = \frac{d}{dp}A'(\eta) \frac{dp}{d\eta} = np(1-p), $$
	$$ \kappa_3 = A'''(\eta) = \frac{d}{dp}A''(\eta) \frac{dp}{d\eta} = (n-2np)p(1-p) = np(1-p)(1-2p). $$
	
	The third moment for $T=X$ can be obtained by
	$$ EX^3 = \kappa_3 + 3\kappa_1\kappa_2 + \kappa_1^3. $$
\end{proof}


%% # 2.27
\section{Problem 2.27}
Let $T$ be a random vector in $\bbR^2$.
\begin{itemize}
	\item[a)] Express $\kappa_{2,1}$ as a function of the moments of $T$.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
            		To make simple derivation, we denote $f_{ij}(u) = \frac{\partial^{i+j}f(u)}{\partial u_1^i \partial u_2^j}$.
			Note that $K(u) = \log M(u)$, by taking derivative,
			$$ K_{10} = \frac{M_{10}}{M}, ~~~ K_{20} = \frac{M_{20}M-M_{10}^2}{M^2}, $$
			\begin{align*}
				K_{21} &= \frac{1}{M^4}\left[ (M_{21}M + M_{20}M_{01}-2M_{10}M_{11})M^2 - 2MM_{01}(M_{20}M-M_{10}^2) \right] \\
					   &= \frac{1}{M^3}[ M_{21}M^2 + M_{20}M_{01}M - 2M_{10}M_{11}M - 2M_{01}M_{20}M-2M_{01}M_{10}^2 ] \\
					   &= \frac{1}{M^3}[ M_{21}M^2 - M_{20}M_{01}M - 2M_{10}M_{11}M -2M_{01}M_{10}^2 ].
			\end{align*}
			Taking $u=0$, then
			$$ \kappa_{2,1} = K_{2,1}|_{u=0} = ET_1^2T_2 - (ET_2)(ET_1^2) - 2(ET_1)(ET_1T_2) -2(ET_2)(ET_1)^2.$$
            	\end{proof}

	\item[b)] Assume $ET_1 = ET_2 = 0$ and give an expression for $\kappa_{2,2}$ in terms of moments of $T$.
            	\begin{proof}[\underline{\textbf{Solution}}] $\newline$
            		$\kappa_{2,2}$ can be obtain by one more derivative for $K_{21}$. 
			{\color{blue} (Too complicated. See Keener page 462.) }
            	\end{proof}
\end{itemize}


%% # 2.28
\section{Problem 2.28}
Suppose $X \sim \Gamma(\alpha, 1/\lambda)$, with density
$$ \frac{\lambda^\alpha x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}, ~~~~ x > 0. $$
Find the cumulants of $T=(X, \log X)$ of order 3 or less.
The answer will involve $\psi(\alpha) = d\log\Gamma(\alpha)/d\alpha = \Gamma'(\alpha)/\Gamma(\alpha)$.

\begin{proof}[\underline{\textbf{Solution}}] $\newline$
	Since Gamma distribution is the exponential family,
	\begin{align*}
		p(x) &= \exp\big[ \alpha \log\lambda + (\alpha -1)\log x - \lambda x - \log \Gamma(\alpha) \big] \\
		       &= \exp\big[ -\lambda x + \alpha \log x - \big(\log\Gamma(\alpha) - \alpha\log \lambda\big) - \log x \big].
	\end{align*}
	Let $\eta = (-\lambda, \alpha)$, and $A(\eta) = \log\Gamma(\eta_2) - \eta_2\log(-\eta_1)$. \\
	Then, the cumulants are obtained by taking derivatives:
	$$ \kappa_{1,0} = \frac{\partial A(\eta)}{\partial \eta_1} = -\frac{\eta_2}{\eta_1} = \frac{\alpha}{\lambda}, $$
	$$ \kappa_{0,1} = \frac{\partial A(\eta)}{\partial \eta_2} = \psi(\eta_2)-\log(-\eta_1) = \psi(\alpha)-\log \lambda, $$
	
	$$ \kappa_{2,0} = \frac{\partial^2 A(\eta)}{\partial \eta_1^2} = \frac{\eta_2}{\eta_1^2} = \frac{\alpha}{\lambda^2}, $$
	$$ \kappa_{1,1} = \frac{\partial^2 A(\eta)}{\partial \eta_1 \partial\eta_2} = -\frac{1}{\eta_1} = \frac{1}{\lambda}, $$
	$$ \kappa_{0,2} = \frac{\partial^2 A(\eta)}{\partial \eta_2^2} = \psi'(\eta_2) = \psi'(\alpha), $$
	
	$$ \kappa_{3,0} = \frac{\partial^3 A(\eta)}{\partial \eta_1^3} = -\frac{2\eta_2}{\eta_1^3} = \frac{2\alpha}{\lambda^3}, $$
	$$ \kappa_{2,1} = \frac{\partial^3 A(\eta)}{\partial \eta_1^2 \partial\eta_2} = \frac{1}{\eta_1^2} = frac{1}{\lambda^2}, $$
	$$ \kappa_{1,2} = \frac{\partial^3 A(\eta)}{\partial \eta_1 \partial\eta_2^2} = 0, $$
	$$ \kappa_{0,3} = \frac{\partial^3 A(\eta)}{\partial \eta_2^3} = \psi''(\eta_2) = \psi''(\alpha). $$
\end{proof}
